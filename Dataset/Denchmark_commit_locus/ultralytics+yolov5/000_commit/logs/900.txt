      Fix redundant outputs via Logging in DDP training (#500)          * Change print to logging          * Clean function set_logging          * Add line spacing          * Change leftover prints to log          * Fix scanning labels output          * Fix rank naming          * Change leftover print to logging          * Reorganized DDP variables          * Fix type error          * Make quotes consistent          * Fix spelling          * Clean function call          * Add line spacing          * Update datasets.py          * Update train.py          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
 from utils.general import (      torch_distributed_zero_first, labels_to_class_weights, plot_labels, check_anchors, labels_to_image_weights,      compute_loss, plot_images, fitness, strip_optimizer, plot_results, get_latest_run, check_dataset, check_file,     check_git_status, check_img_size, increment_dir, print_mutation, plot_evolution)     check_git_status, check_img_size, increment_dir, print_mutation, plot_evolution, set_logging)  from utils.google_utils import attempt_download  from utils.torch_utils import init_seeds, ModelEMA, select_device, intersect_dicts   logger = logging.getLogger(__name__)    def train(hyp, opt, device, tb_writer=None):     print(f'Hyperparameters {hyp}')     logger.info(f'Hyperparameters {hyp}')      log_dir = Path(tb_writer.log_dir) if tb_writer else Path(opt.logdir) / 'evolve'  # logging directory      wdir = str(log_dir / 'weights') + os.sep  # weights directory      os.makedirs(wdir, exist_ok=True) 
