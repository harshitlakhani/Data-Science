      Add TransformerLayer, TransformerBlock, C3TR modules (#2333)          * yolotr          * transformer block          * Remove bias in Transformer          * Remove C3T          * Remove a deprecated class          * put the 2nd LayerNorm into the 2nd residual block          * move example model to models/hub, rename to -transformer          * Add module comments and TODOs          * Remove LN in Transformer          * Add comments for Transformer          * Solve the problem of MA with DDP          * cleanup          * cleanup find_unused_parameters          * PEP8 reformat          Co-authored-by: DingYiwei <846414640@qq.com>     Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
           n = max(round(n * gd), 1) if n > 1 else n  # depth gain          if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP,                  C3]:                  C3, C3TR]:              c1, c2 = ch[f], args[0]              if c2 != no:  # if not output                  c2 = make_divisible(c2 * gw, 8)                args = [c1, c2, *args[1:]]             if m in [BottleneckCSP, C3]:             if m in [BottleneckCSP, C3, C3TR]:                  args.insert(2, n)  # number of repeats                  n = 1          elif m is nn.BatchNorm2d: 
