      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
     maps = np.zeros(nc)  # mAP per class      results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'      scheduler.last_epoch = start_epoch - 1  # do not move     scaler = amp.GradScaler(enabled=cuda)      if rank in [0, -1]:          print('Image sizes %g train, %g test' % (imgsz, imgsz_test))          print('Using %g dataloader workers' % dataloader.num_workers) 
