      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
           del ckpt       # Mixed precision training https://github.com/NVIDIA/apex     if mixed_precision:         model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)       # DP mode     if device.type != 'cpu' and rank == -1 and torch.cuda.device_count() > 1:     if cuda and rank == -1 and torch.cuda.device_count() > 1:          model = torch.nn.DataParallel(model)        # SyncBatchNorm     if opt.sync_bn and device.type != 'cpu' and rank != -1:     if opt.sync_bn and cuda and rank != -1:          model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)          print('Using SyncBatchNorm()')   
