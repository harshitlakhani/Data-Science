      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
     parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')      opt = parser.parse_args()       # Resume      last = get_latest_run() if opt.resume == 'get_last' else opt.resume  # resume from most recent run      if last and not opt.weights:          print(f'Resuming training from {last}')      opt.weights = last if opt.resume and not opt.weights else opt.weights       if opt.local_rank in [-1, 0]:          check_git_status()      opt.cfg = check_file(opt.cfg)  # check file 
