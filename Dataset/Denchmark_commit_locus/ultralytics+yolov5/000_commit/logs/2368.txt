      reorganize train initialization steps 
     nb = len(dataloader)  # number of batches      assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)       # Testloader     # Process 0      if rank in [-1, 0]:          ema.updates = start_epoch * nb // accumulate  # set EMA updates          testloader = create_dataloader(test_path, imgsz_test, total_batch_size, gs, opt,                                         hyp=hyp, augment=False, cache=opt.cache_images, rect=True, rank=-1,                                        world_size=opt.world_size, workers=opt.workers)[0]  # only runs on process 0                                        world_size=opt.world_size, workers=opt.workers)[0]  # testloader          if not opt.resume:             labels = np.concatenate(dataset.labels, 0)             c = torch.tensor(labels[:, 0])  # classes             # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency             # model._initialize_biases(cf.to(device))             plot_labels(labels, save_dir=log_dir)             if tb_writer:                 # tb_writer.add_hparams(hyp, {})  # causes duplicate https://github.com/ultralytics/yolov5/pull/384                 tb_writer.add_histogram('classes', c, 0)              # Anchors             if not opt.noautoanchor:                 check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)        # Model parameters      hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset 
