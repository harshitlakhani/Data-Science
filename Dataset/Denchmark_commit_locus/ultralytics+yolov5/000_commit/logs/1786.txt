      W&B logging add hyperparameters (#1399)          * W&B logging add hyperparameters          * hyp bug fix and image logging updates          * if plots and wandb:          * cleanup          * wandb/ gitignore add          * cleanup 2          * cleanup 3          * move wandb import to top of file          * wandb evolve          * update import          * wandb.run.finish()          * default anchors: 3 
         model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)          logger.info('Using SyncBatchNorm()')       # Exponential moving average     # EMA      ema = ModelEMA(model) if rank in [-1, 0] else None        # DDP mode 
