      Improved model+EMA checkpointing (#2292)          * Enhanced model+EMA checkpointing          * update          * bug fix          * bug fix 2          * always save optimizer          * ema half          * remove model.float()          * model half          * carry ema/model in fp32          * rm model.float()          * both to float always          * cleanup          * cleanup 
     if not training:          s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''          print(f"Results saved to {save_dir}{s}")     model.float()  # for training      maps = np.zeros(nc) + map      for i, c in enumerate(ap_class):          maps[c] = ap[i] 
