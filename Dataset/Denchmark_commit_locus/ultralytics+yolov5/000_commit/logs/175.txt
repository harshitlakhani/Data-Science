      switch default inference to FP16 on GPU 
     # Initialize/load model and set device      if model is None:          device = torch_utils.select_device(opt.device, batch_size=batch_size)         half &= device.type != 'cpu'  # half precision only supported on CUDA         half = device.type != 'cpu'  # half precision only supported on CUDA            # Remove previous          for f in glob.glob('test_batch*.jpg'): 
