      Fix redundant outputs via Logging in DDP training (#500)          * Change print to logging          * Clean function set_logging          * Add line spacing          * Change leftover prints to log          * Fix scanning labels output          * Fix rank naming          * Change leftover print to logging          * Reorganized DDP variables          * Fix type error          * Make quotes consistent          * Fix spelling          * Clean function call          * Add line spacing          * Update datasets.py          * Update train.py          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
                                       cache_images=cache,                                        single_cls=opt.single_cls,                                        stride=int(stride),                                       pad=pad)                                       pad=pad,                                       rank=rank)        batch_size = min(batch_size, len(dataset))      nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, 8])  # number of workers     train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) if local_rank != -1 else None     train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None      dataloader = torch.utils.data.DataLoader(dataset,                                               batch_size=batch_size,                                               num_workers=nw, 
