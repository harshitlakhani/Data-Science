      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
                                       prefix=prefix)        batch_size = min(batch_size, len(dataset))     nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, workers])  # number of workers     nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, workers])  # number of workers      sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None      loader = torch.utils.data.DataLoader if image_weights else InfiniteDataLoader      # Use torch.utils.data.DataLoader() if dataset.properties will update during training else InfiniteDataLoader() 
