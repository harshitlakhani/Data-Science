      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
             tb_writer = SummaryWriter(log_dir=increment_dir('runs/exp', opt.name))          else:              tb_writer = None           train(hyp, tb_writer, opt, device)        # Evolve hyperparameters (optional)      else:         assert opt.local_rank == -1, "DDP mode currently not implemented for Evolve!"         assert opt.local_rank == -1, 'DDP mode not implemented for --evolve'            tb_writer = None          opt.notest, opt.nosave = True, True  # only test/save final epoch 
