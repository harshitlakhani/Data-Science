      Add TransformerLayer, TransformerBlock, C3TR modules (#2333)          * yolotr          * transformer block          * Remove bias in Transformer          * Remove C3T          * Remove a deprecated class          * put the 2nd LayerNorm into the 2nd residual block          * move example model to models/hub, rename to -transformer          * Add module comments and TODOs          * Remove LN in Transformer          * Add comments for Transformer          * Solve the problem of MA with DDP          * cleanup          * cleanup find_unused_parameters          * PEP8 reformat          Co-authored-by: DingYiwei <846414640@qq.com>     Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
         return self.act(self.conv(x))     class TransformerLayer(nn.Module):     # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)     def __init__(self, c, num_heads):         super().__init__()         self.q = nn.Linear(c, c, bias=False)         self.k = nn.Linear(c, c, bias=False)         self.v = nn.Linear(c, c, bias=False)         self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)         self.fc1 = nn.Linear(c, c, bias=False)         self.fc2 = nn.Linear(c, c, bias=False)      def forward(self, x):         x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x         x = self.fc2(self.fc1(x)) + x         return x   class TransformerBlock(nn.Module):     # Vision Transformer https://arxiv.org/abs/2010.11929     def __init__(self, c1, c2, num_heads, num_layers):         super().__init__()         self.conv = None         if c1 != c2:             self.conv = Conv(c1, c2)         self.linear = nn.Linear(c2, c2)  # learnable position embedding         self.tr = nn.Sequential(*[TransformerLayer(c2, num_heads) for _ in range(num_layers)])         self.c2 = c2      def forward(self, x):         if self.conv is not None:             x = self.conv(x)         b, _, w, h = x.shape         p = x.flatten(2)         p = p.unsqueeze(0)         p = p.transpose(0, 3)         p = p.squeeze(3)         e = self.linear(p)         x = p + e          x = self.tr(x)         x = x.unsqueeze(3)         x = x.transpose(0, 3)         x = x.reshape(b, self.c2, w, h)         return x    class Bottleneck(nn.Module):      # Standard bottleneck      def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion 
