      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
         if ng > 1 and batch_size:  # check that batch_size is compatible with device_count              assert batch_size % ng == 0, 'batch-size %g not multiple of GPU count %g' % (batch_size, ng)          x = [torch.cuda.get_device_properties(i) for i in range(ng)]         s = 'Using CUDA ' + ('Apex ' if apex else '')  # apex for mixed precision https://github.com/NVIDIA/apex         s = 'Using CUDA '          for i in range(0, ng):              if i == 1:                  s = ' ' * len(s) 
