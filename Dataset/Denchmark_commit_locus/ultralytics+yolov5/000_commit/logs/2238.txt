      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
         yaml.dump(vars(opt), f, sort_keys=False)        # Configure     cuda = device.type != 'cpu'      init_seeds(2 + rank)      with open(opt.data) as f:          data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict 
