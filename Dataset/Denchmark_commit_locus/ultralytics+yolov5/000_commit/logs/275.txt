      Change optimizer parameters group method (#1239)          * Change optimizer parameters group method          * Add torch nn          * Change isinstance method(torch.Tensor to nn.Parameter)          * parameter freeze fix, PEP8 reformat          * freeze bug fix          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
     hyp['weight_decay'] *= total_batch_size * accumulate / nbs  # scale weight_decay        pg0, pg1, pg2 = [], [], []  # optimizer parameter groups     for k, v in model.named_parameters():         v.requires_grad = True         if '.bias' in k:             pg2.append(v)  # biases         elif '.weight' in k and '.bn' not in k:             pg1.append(v)  # apply weight decay         else:             pg0.append(v)  # all else     for k, v in model.named_modules():         if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):             pg2.append(v.bias)  # biases         if isinstance(v, nn.BatchNorm2d):             pg0.append(v.weight)  # no decay         elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):             pg1.append(v.weight)  # apply decay        if opt.adam:          optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum 
