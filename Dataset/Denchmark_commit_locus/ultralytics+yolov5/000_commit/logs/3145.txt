      Utils reorganization (#1392)          * Utils reorganization          * Add new utils files          * cleanup          * simplify          * reduce datasets.py          * remove evolve.sh          * loadWebcam cleanup 
 cv2.setNumThreads(0)     @contextmanager def torch_distributed_zero_first(local_rank: int):     """     Decorator to make all processes in distributed training wait for each local_master to do something.     """     if local_rank not in [-1, 0]:         torch.distributed.barrier()     yield     if local_rank == 0:         torch.distributed.barrier()    def set_logging(rank=-1):      logging.basicConfig(          format="%(message)s", 
