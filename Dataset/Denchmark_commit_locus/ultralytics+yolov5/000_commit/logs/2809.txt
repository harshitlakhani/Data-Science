      Improved model+EMA checkpointing (#2292)          * Enhanced model+EMA checkpointing          * update          * bug fix          * bug fix 2          * always save optimizer          * ema half          * remove model.float()          * model half          * carry ema/model in fp32          * rm model.float()          * both to float always          * cleanup          * cleanup 
                                id=ckpt.get('wandb_id') if 'ckpt' in locals() else None)      loggers = {'wandb': wandb}  # loggers dict       # EMA     ema = ModelEMA(model) if rank in [-1, 0] else None       # Resume      start_epoch, best_fitness = 0, 0.0      if pretrained: 
