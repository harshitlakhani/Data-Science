      Fix redundant outputs via Logging in DDP training (#500)          * Change print to logging          * Clean function set_logging          * Add line spacing          * Change leftover prints to log          * Fix scanning labels output          * Fix rank naming          * Change leftover print to logging          * Reorganized DDP variables          * Fix type error          * Make quotes consistent          * Fix spelling          * Clean function call          * Add line spacing          * Update datasets.py          * Update train.py          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
     # SyncBatchNorm      if opt.sync_bn and cuda and rank != -1:          model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)         print('Using SyncBatchNorm()')         logger.info('Using SyncBatchNorm()')        # Exponential moving average      ema = ModelEMA(model) if rank in [-1, 0] else None 
