      Fix redundant outputs via Logging in DDP training (#500)          * Change print to logging          * Clean function set_logging          * Add line spacing          * Change leftover prints to log          * Fix scanning labels output          * Fix rank naming          * Change leftover print to logging          * Reorganized DDP variables          * Fix type error          * Make quotes consistent          * Fix spelling          * Clean function call          * Add line spacing          * Update datasets.py          * Update train.py          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
     def create_dataloader(path, imgsz, batch_size, stride, opt, hyp=None, augment=False, cache=False, pad=0.0, rect=False,                       local_rank=-1, world_size=1):                       rank=-1, world_size=1):      # Make sure only the first process in DDP process the dataset first, and the following others can use the cache.     with torch_distributed_zero_first(local_rank):     with torch_distributed_zero_first(rank):          dataset = LoadImagesAndLabels(path, imgsz, batch_size,                                        augment=augment,  # augment images                                        hyp=hyp,  # augmentation hyperparameters 
