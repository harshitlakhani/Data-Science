      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
         # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders            mloss = torch.zeros(4, device=device)  # mean losses         if rank != -1:         if RANK != -1:              dataloader.sampler.set_epoch(epoch)          pbar = enumerate(dataloader)          logger.info(('\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'labels', 'img_size'))         if rank in [-1, 0]:         if RANK in [-1, 0]:              pbar = tqdm(pbar, total=nb)  # progress bar          optimizer.zero_grad()          for i, (imgs, targets, paths, _) in pbar:  # batch ------------------------------------------------------------- 
