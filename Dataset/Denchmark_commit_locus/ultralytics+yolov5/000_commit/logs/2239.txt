      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
     optimizer.add_param_group({'params': pg2})  # add pg2 (biases)      print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))      del pg0, pg1, pg2            # Scheduler https://arxiv.org/pdf/1812.01187.pdf      lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.8 + 0.2  # cosine      scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf) 
