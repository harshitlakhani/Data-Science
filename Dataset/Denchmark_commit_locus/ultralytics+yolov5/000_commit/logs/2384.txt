      hyp evolution force-autoanchor fix 
         with torch_distributed_zero_first(rank):              attempt_download(weights)  # download if not found locally          ckpt = torch.load(weights, map_location=device)  # load checkpoint         if 'anchors' in hyp and hyp['anchors']:         if hyp.get('anchors'):              ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  # force autoanchor          model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  # create         exclude = ['anchor'] if opt.cfg else []  # exclude keys         exclude = ['anchor'] if opt.cfg or hyp.get('anchors') else []  # exclude keys          state_dict = ckpt['model'].float().state_dict()  # to FP32          state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect          model.load_state_dict(state_dict, strict=False)  # load 
