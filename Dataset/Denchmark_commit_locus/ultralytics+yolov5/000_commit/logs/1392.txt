      FP16 to FP32 ckpt load 
           # Load model          google_utils.attempt_download(weights)         model = torch.load(weights, map_location=device)['model']         model = torch.load(weights, map_location=device)['model'].float()  # load to FP32          torch_utils.model_info(model)          # model.fuse()          model.to(device) 
