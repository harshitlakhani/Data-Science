      Eliminate `total_batch_size` variable (#3697)          * Eliminate `total_batch_size` variable          * cleanup          * Update train.py 
             if ni <= nw:                  xi = [0, nw]  # x interp                  # model.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)                 accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())                 accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())                  for j, x in enumerate(optimizer.param_groups):                      # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0                      x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)]) 
