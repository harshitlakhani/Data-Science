      Improved W&B integration  (#2125)          * Init Commit          * new wandb integration          * Update          * Use data_dict in test          * Updates          * Update: scope of log_img          * Update: scope of log_img          * Update          * Update: Fix logging conditions          * Add tqdm bar, support for .txt dataset format          * Improve Result table Logger          * Init Commit          * new wandb integration          * Update          * Use data_dict in test          * Updates          * Update: scope of log_img          * Update: scope of log_img          * Update          * Update: Fix logging conditions          * Add tqdm bar, support for .txt dataset format          * Improve Result table Logger          * Add dataset creation in training script          * Change scope: self.wandb_run          * Add wandb-artifact:// natively          you can now use --resume with wandb run links          * Add suuport for logging dataset while training          * Cleanup          * Fix: Merge conflict          * Fix: CI tests          * Automatically use wandb config          * Fix: Resume          * Fix: CI          * Enhance: Using val_table          * More resume enhancement          * FIX : CI          * Add alias          * Get useful opt config data          * train.py cleanup          * Cleanup train.py          * more cleanup          * Cleanup| CI fix          * Reformat using PEP8          * FIX:CI          * rebase          * remove uneccesary changes          * remove uneccesary changes          * remove uneccesary changes          * remove unecessary chage from test.py          * FIX: resume from local checkpoint          * FIX:resume          * FIX:resume          * Reformat          * Performance improvement          * Fix local resume          * Fix local resume          * FIX:CI          * Fix: CI          * Imporve image logging          * (:(:Redo CI tests:):)          * Remember epochs when resuming          * Remember epochs when resuming          * Update DDP location          Potential fix for #2405          * PEP8 reformat          * 0.25 confidence threshold          * reset train.py plots syntax to previous          * reset epochs completed syntax to previous          * reset space to previous          * remove brackets          * reset comment to previous          * Update: is_coco check, remove unused code          * Remove redundant print statement          * Remove wandb imports          * remove dsviz logger from test.py          * Remove redundant change from test.py          * remove redundant changes from train.py          * reformat and improvements          * Fix typo          * Add tqdm tqdm progress when scanning files, naming improvements          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
     init_seeds(2 + rank)      with open(opt.data) as f:          data_dict = yaml.load(f, Loader=yaml.SafeLoader)  # data dict     with torch_distributed_zero_first(rank):         check_dataset(data_dict)  # check     train_path = data_dict['train']     test_path = data_dict['val']     is_coco = opt.data.endswith('coco.yaml')      # Logging- Doing this before checking the dataset. Might update data_dict     if rank in [-1, 0]:         opt.hyp = hyp  # add hyperparameters         run_id = torch.load(weights).get('wandb_id') if weights.endswith('.pt') and os.path.isfile(weights) else None         wandb_logger = WandbLogger(opt, Path(opt.save_dir).stem, run_id, data_dict)         data_dict = wandb_logger.data_dict         if wandb_logger.wandb:             weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp  # WandbLogger might update weights, epochs if resuming     loggers = {'wandb': wandb_logger.wandb}  # loggers dict      nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes      names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names      assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  # check 
