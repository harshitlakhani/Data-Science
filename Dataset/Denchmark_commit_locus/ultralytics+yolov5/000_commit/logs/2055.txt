      Eliminate `total_batch_size` variable (#3697)          * Eliminate `total_batch_size` variable          * cleanup          * Update train.py 
         opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok | opt.evolve))        # DDP mode     opt.total_batch_size = opt.batch_size      device = select_device(opt.device, batch_size=opt.batch_size)      if LOCAL_RANK != -1:          from datetime import timedelta         assert torch.cuda.device_count() > LOCAL_RANK, 'too few GPUS for DDP command'         assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'          torch.cuda.set_device(LOCAL_RANK)          device = torch.device('cuda', LOCAL_RANK)          dist.init_process_group(backend="gloo", timeout=timedelta(seconds=60))          assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'          assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'         opt.batch_size = opt.total_batch_size // WORLD_SIZE        # Train      if not opt.evolve: 
