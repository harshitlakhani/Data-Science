      Merge branch 'master' into advanced_logging 
              else:                   pg0.append(v)  # all else     -    optimizer = optim.Adam(pg0, lr=hyp['lr0']) if opt.adam else \  -        optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)  +    if hyp['optimizer'] =='adam':  +        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999)) #use default beta2, adjust beta1 for Adam momentum per momentum adjustments in https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR  +    else:  +        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)  +       optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay       optimizer.add_param_group({'params': pg2})  # add pg2 (biases)      # Scheduler https://arxiv.org/pdf/1812.01187.pdf      lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine      scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)       print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))       del pg0, pg1, pg2    
