      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
                     ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)                      imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)               # Forward             pred = model(imgs)             # Autocast             with amp.autocast():                 # Forward                 pred = model(imgs)               # Loss             loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size             if rank != -1:                 loss *= opt.world_size  # gradient averaged between devices in DDP mode             if not torch.isfinite(loss):                 print('WARNING: non-finite loss, ending training ', loss_items)                 return results                 # Loss                 loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size                 if rank != -1:                     loss *= opt.world_size  # gradient averaged between devices in DDP mode                 # if not torch.isfinite(loss):                 #     print('WARNING: non-finite loss, ending training ', loss_items)                 #     return results                # Backward             if mixed_precision:                 with amp.scale_loss(loss, optimizer) as scaled_loss:                     scaled_loss.backward()             else:                 loss.backward()             scaler.scale(loss).backward()                # Optimize              if ni % accumulate == 0:                 optimizer.step()                 scaler.step(optimizer)  # optimizer.step                 scaler.update()                  optimizer.zero_grad()                  if ema is not None:                      ema.update(model) 
