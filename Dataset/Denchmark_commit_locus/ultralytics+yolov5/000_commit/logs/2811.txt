      Improved model+EMA checkpointing (#2292)          * Enhanced model+EMA checkpointing          * update          * bug fix          * bug fix 2          * always save optimizer          * ema half          * remove model.float()          * model half          * carry ema/model in fp32          * rm model.float()          * both to float always          * cleanup          * cleanup 
         model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)          logger.info('Using SyncBatchNorm()')       # EMA     ema = ModelEMA(model) if rank in [-1, 0] else None       # DDP mode      if cuda and rank != -1:          model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank) 
