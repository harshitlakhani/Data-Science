      Update datasets.py (#494) 
     return s     def create_dataloader(path, imgsz, batch_size, stride, opt, hyp=None, augment=False, cache=False, pad=0.0, rect=False, local_rank=-1, world_size=1): def create_dataloader(path, imgsz, batch_size, stride, opt, hyp=None, augment=False, cache=False, pad=0.0, rect=False,                       local_rank=-1, world_size=1):      # Make sure only the first process in DDP process the dataset first, and the following others can use the cache.      with torch_distributed_zero_first(local_rank):          dataset = LoadImagesAndLabels(path, imgsz, batch_size,                                     augment=augment,  # augment images                                     hyp=hyp,  # augmentation hyperparameters                                     rect=rect,  # rectangular training                                     cache_images=cache,                                     single_cls=opt.single_cls,                                     stride=int(stride),                                     pad=pad)                                       augment=augment,  # augment images                                       hyp=hyp,  # augmentation hyperparameters                                       rect=rect,  # rectangular training                                       cache_images=cache,                                       single_cls=opt.single_cls,                                       stride=int(stride),                                       pad=pad)        batch_size = min(batch_size, len(dataset))      nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, 8])  # number of workers 
