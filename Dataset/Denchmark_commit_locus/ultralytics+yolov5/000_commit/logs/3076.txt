      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
     parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')      parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')      parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')     parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')      parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')      parser.add_argument('--project', default='runs/train', help='save to project/name')      parser.add_argument('--entity', default=None, help='W&B entity') 
