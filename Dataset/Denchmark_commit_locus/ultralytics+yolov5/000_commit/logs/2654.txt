      Merge branch 'master' into advanced_logging 
      model.hyp = hyp  # attach hyperparameters to model       model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)       model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights      model.names = data_dict['names']     +    #save hyperparamter and training options in run folder  +    with open(os.path.join(log_dir, 'hyp.yaml'), 'w') as f:  +        yaml.dump(hyp, f, sort_keys=False)  +  +    with open(os.path.join(log_dir, 'opt.yaml'), 'w') as f:  +        yaml.dump(vars(opt), f, sort_keys=False)  +           # Class frequency       labels = np.concatenate(dataset.labels, 0)       c = torch.tensor(labels[:, 0])  # classes       # cf = torch.bincount(c.long(), minlength=nc) + 1.       # model._initialize_biases(cf.to(device)) + +    #always plot labels to log_dir  +    plot_labels(labels, save_dir=log_dir)      tb_writer.add_histogram('classes', c, 0) +      if tb_writer:  -        plot_labels(labels)          tb_writer.add_histogram('classes', c, 0)    +       # Check anchors      check_best_possible_recall(dataset, anchors=model.model[-1].anchor_grid, thr=hyp['anchor_t'], imgsz=imgsz)      if not opt.noautoanchor:          check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)          # Exponential moving average       ema = torch_utils.ModelEMA(model) 
