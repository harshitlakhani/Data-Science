      max workers for dataloader (#722) 
                                       rank=rank)        batch_size = min(batch_size, len(dataset))     nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, 8])  # number of workers     nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, workers])  # number of workers      train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None      dataloader = torch.utils.data.DataLoader(dataset,                                               batch_size=batch_size, 
