      Update torch_utils.py 
         self.updates += 1          d = self.decay(self.updates)          with torch.no_grad():             if type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel):             if is_parallel(model):                  msd, esd = model.module.state_dict(), self.ema.module.state_dict()              else:                  msd, esd = model.state_dict(), self.ema.state_dict() 
