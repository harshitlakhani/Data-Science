      Fix redundant outputs via Logging in DDP training (#500)          * Change print to logging          * Clean function set_logging          * Add line spacing          * Change leftover prints to log          * Fix scanning labels output          * Fix rank naming          * Change leftover print to logging          * Reorganized DDP variables          * Fix type error          * Make quotes consistent          * Fix spelling          * Clean function call          * Add line spacing          * Update datasets.py          * Update train.py          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
         torch.cuda.set_device(opt.local_rank)          device = torch.device('cuda', opt.local_rank)          dist.init_process_group(backend='nccl', init_method='env://')  # distributed backend         opt.world_size = dist.get_world_size()         opt.global_rank = dist.get_rank()          assert opt.batch_size % opt.world_size == 0, '--batch-size must be multiple of CUDA device count'          opt.batch_size = opt.total_batch_size // opt.world_size       print(opt)     logger.info(opt)      with open(opt.hyp) as f:          hyp = yaml.load(f, Loader=yaml.FullLoader)  # load hyps   
