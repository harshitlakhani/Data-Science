      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
         model.train()            # Update image weights (optional)         # When in DDP mode, the generated indices will be broadcasted to synchronize dataset.          if dataset.image_weights:             # Generate indices.             # Generate indices              if rank in [-1, 0]:                  w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights                  image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)                  dataset.indices = random.choices(range(dataset.n), weights=image_weights,                                                   k=dataset.n)  # rand weighted idx             # Broadcast.             # Broadcast if DDP              if rank != -1:                  indices = torch.zeros([dataset.n], dtype=torch.int)                  if rank == 0: 
