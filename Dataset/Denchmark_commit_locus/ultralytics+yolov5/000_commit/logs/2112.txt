      torch.cuda.amp bug fix (#2750)          PR https://github.com/ultralytics/yolov5/pull/2725 introduced a very specific bug that only affects multi-GPU trainings. Apparently the cause was using the torch.cuda.amp decorator in the autoShape forward method. I've implemented amp more traditionally in this PR, and the bug is resolved. 
         return self        @torch.no_grad()     @torch.cuda.amp.autocast(torch.cuda.is_available())      def forward(self, imgs, size=640, augment=False, profile=False):          # Inference from various sources. For height=640, width=1280, RGB images example inputs are:          #   filename:   imgs = 'data/samples/zidane.jpg' 
