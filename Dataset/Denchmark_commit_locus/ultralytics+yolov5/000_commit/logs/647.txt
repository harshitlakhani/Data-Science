      Fix batch-size on resume for multi-gpu (#1942) 
         apriori = opt.global_rank, opt.local_rank          with open(Path(ckpt).parent.parent / 'opt.yaml') as f:              opt = argparse.Namespace(**yaml.load(f, Loader=yaml.FullLoader))  # replace         opt.cfg, opt.weights, opt.resume, opt.global_rank, opt.local_rank = '', ckpt, True, *apriori  # reinstate         opt.cfg, opt.weights, opt.resume, opt.batch_size, opt.global_rank, opt.local_rank = '', ckpt, True, opt.total_batch_size, *apriori  # reinstate          logger.info('Resuming training from %s' % ckpt)      else:          # opt.hyp = opt.hyp or ('hyp.finetune.yaml' if opt.weights else 'hyp.scratch.yaml') 
