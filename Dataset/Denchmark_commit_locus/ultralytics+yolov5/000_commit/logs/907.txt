      Fix redundant outputs via Logging in DDP training (#500)          * Change print to logging          * Clean function set_logging          * Add line spacing          * Change leftover prints to log          * Fix scanning labels output          * Fix rank naming          * Change leftover print to logging          * Reorganized DDP variables          * Fix type error          * Make quotes consistent          * Fix spelling          * Clean function call          * Add line spacing          * Update datasets.py          * Update train.py          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
     results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'      scheduler.last_epoch = start_epoch - 1  # do not move      scaler = amp.GradScaler(enabled=cuda)     if rank in [0, -1]:         print('Image sizes %g train, %g test' % (imgsz, imgsz_test))         print('Using %g dataloader workers' % dataloader.num_workers)         print('Starting training for %g epochs...' % epochs)     logger.info('Image sizes %g train, %g test' % (imgsz, imgsz_test))     logger.info('Using %g dataloader workers' % dataloader.num_workers)     logger.info('Starting training for %g epochs...' % epochs)      # torch.autograd.set_detect_anomaly(True)      for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------          model.train() 
