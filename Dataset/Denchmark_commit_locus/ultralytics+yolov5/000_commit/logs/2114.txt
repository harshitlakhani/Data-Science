      torch.cuda.amp bug fix (#2750)          PR https://github.com/ultralytics/yolov5/pull/2725 introduced a very specific bug that only affects multi-GPU trainings. Apparently the cause was using the torch.cuda.amp decorator in the autoShape forward method. I've implemented amp more traditionally in this PR, and the bug is resolved. 
         x = torch.from_numpy(x).to(p.device).type_as(p) / 255.  # uint8 to fp16/32          t.append(time_synchronized())           # Inference         y = self.model(x, augment, profile)[0]  # forward         t.append(time_synchronized())         with amp.autocast(enabled=p.device.type != 'cpu'):             # Inference             y = self.model(x, augment, profile)[0]  # forward             t.append(time_synchronized())           # Post-process         y = non_max_suppression(y, conf_thres=self.conf, iou_thres=self.iou, classes=self.classes)  # NMS         for i in range(n):             scale_coords(shape1, y[i][:, :4], shape0[i])             # Post-process             y = non_max_suppression(y, conf_thres=self.conf, iou_thres=self.iou, classes=self.classes)  # NMS             for i in range(n):                 scale_coords(shape1, y[i][:, :4], shape0[i])           t.append(time_synchronized())         return Detections(imgs, y, files, t, self.names, x.shape)             t.append(time_synchronized())             return Detections(imgs, y, files, t, self.names, x.shape)      class Detections: 
