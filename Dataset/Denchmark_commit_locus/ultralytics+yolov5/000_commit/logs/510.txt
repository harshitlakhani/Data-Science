      ACON activation function (#2893)          * ACON Activation Function          ## �윓� Feature          There is a new activation function [ACON (CVPR 2021)](https://arxiv.org/pdf/2009.04759.pdf) that unifies ReLU and Swish.     ACON is simple but very effective, code is here: https://github.com/nmaac/acon/blob/main/acon.py#L19          ![image](https://user-images.githubusercontent.com/5032208/115676962-a38dfe80-a382-11eb-9883-61fa3216e3e6.png)          The improvements are very significant:     ![image](https://user-images.githubusercontent.com/5032208/115680180-eac9be80-a385-11eb-9c7a-8643db552c69.png)          ## Alternatives          It also has an enhanced version meta-ACON that uses a small network to learn beta explicitly, which may influence the speed a bit.          ## Additional context          [Code](https://github.com/nmaac/acon) and [paper](https://arxiv.org/pdf/2009.04759.pdf).          * Update activations.py 
         return x * F.hardtanh(x + 3, 0., 6.) / 6.  # for torchscript, CoreML and ONNX     class MemoryEfficientSwish(nn.Module):     class F(torch.autograd.Function):         @staticmethod         def forward(ctx, x):             ctx.save_for_backward(x)             return x * torch.sigmoid(x)          @staticmethod         def backward(ctx, grad_output):             x = ctx.saved_tensors[0]             sx = torch.sigmoid(x)             return grad_output * (sx * (1 + x * (1 - sx)))      def forward(self, x):         return self.F.apply(x)    # Mish https://github.com/digantamisra98/Mish --------------------------------------------------------------------------  class Mish(nn.Module):      @staticmethod 
