      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
           with open(opt.hyp) as f:              hyp = yaml.safe_load(f)  # load hyps dict         assert opt.local_rank == -1, 'DDP mode not implemented for --evolve'         assert LOCAL_RANK == -1, 'DDP mode not implemented for --evolve'          opt.notest, opt.nosave = True, True  # only test/save final epoch          # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices          yaml_file = Path(opt.save_dir) / 'hyp_evolved.yaml'  # save best result here 
