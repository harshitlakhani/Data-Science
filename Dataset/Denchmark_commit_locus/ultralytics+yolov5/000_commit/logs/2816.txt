      Improved model+EMA checkpointing (#2292)          * Enhanced model+EMA checkpointing          * update          * bug fix          * bug fix 2          * always save optimizer          * ema half          * remove model.float()          * model half          * carry ema/model in fp32          * rm model.float()          * both to float always          * cleanup          * cleanup 
 def strip_optimizer(f='weights/best.pt', s=''):  # from utils.general import *; strip_optimizer()      # Strip optimizer from 'f' to finalize training, optionally save as 's'      x = torch.load(f, map_location=torch.device('cpu'))     for key in 'optimizer', 'training_results', 'wandb_id':         x[key] = None     for k in 'optimizer', 'training_results', 'wandb_id', 'ema':  # keys         x[k] = None      x['epoch'] = -1      x['model'].half()  # to FP16      for p in x['model'].parameters(): 
