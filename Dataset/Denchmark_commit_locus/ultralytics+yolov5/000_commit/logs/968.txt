      LR --resume repeat bug fix (#565) 
     optimizer.add_param_group({'params': pg2})  # add pg2 (biases)      print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))      del pg0, pg1, pg2          # Scheduler https://arxiv.org/pdf/1812.01187.pdf     lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.8 + 0.2  # cosine     scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)     # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822     # plot_lr_scheduler(optimizer, scheduler, epochs)        # Load Model      with torch_distributed_zero_first(rank): 
