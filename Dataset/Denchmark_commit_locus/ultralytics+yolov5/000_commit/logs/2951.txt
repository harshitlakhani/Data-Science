      Merge branch 'master' into advanced_logging 
      if mixed_precision:           model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)         # Scheduler https://arxiv.org/pdf/1812.01187.pdf      lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine      scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)         scheduler.last_epoch = start_epoch - 1  # do not move       # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822  -    # plot_lr_scheduler(optimizer, scheduler, epochs)  +    plot_lr_scheduler(optimizer, scheduler, epochs, save_dir = log_dir)          # Initialize distributed training       if device.type != 'cpu' and torch.cuda.device_count() > 1 and torch.distributed.is_available(): 
