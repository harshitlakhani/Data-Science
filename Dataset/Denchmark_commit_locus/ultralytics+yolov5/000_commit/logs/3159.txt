      Utils reorganization (#1392)          * Utils reorganization          * Add new utils files          * cleanup          * simplify          * reduce datasets.py          * remove evolve.sh          * loadWebcam cleanup 
 logger = logging.getLogger(__name__)     def init_torch_seeds(seed=0):     torch.manual_seed(seed) @contextmanager def torch_distributed_zero_first(local_rank: int):     """     Decorator to make all processes in distributed training wait for each local_master to do something.     """     if local_rank not in [-1, 0]:         torch.distributed.barrier()     yield     if local_rank == 0:         torch.distributed.barrier()    def init_torch_seeds(seed=0):      # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html     torch.manual_seed(seed)      if seed == 0:  # slower, more reproducible          cudnn.deterministic = True          cudnn.benchmark = False 
