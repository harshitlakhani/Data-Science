      FP16 inference update 
       seen = 0      model.eval()     _ = model(torch.zeros((1, 3, imgsz, imgsz), device=device)) if device.type != 'cpu' else None  # run once     img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img     _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once      names = model.names if hasattr(model, 'names') else model.module.names      coco91class = coco80_to_coco91_class()      s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')      p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.      loss = torch.zeros(3, device=device)      jdict, stats, ap, ap_class = [], [], [], []     for batch_i, (imgs, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):         imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0     for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):         img = img.to(device)         img = img.half() if half else img.float()  # uint8 to fp16/32         img /= 255.0  # 0 - 255 to 0.0 - 1.0          targets = targets.to(device)         nb, _, height, width = imgs.shape  # batch size, channels, height, width         nb, _, height, width = img.shape  # batch size, channels, height, width          whwh = torch.Tensor([width, height, width, height]).to(device)            # Disable gradients          with torch.no_grad():              # Run model              t = torch_utils.time_synchronized()             inf_out, train_out = model(imgs, augment=augment)  # inference and training outputs             inf_out, train_out = model(img, augment=augment)  # inference and training outputs              t0 += torch_utils.time_synchronized() - t                # Compute loss 
