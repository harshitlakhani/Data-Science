      max workers for dataloader (#722) 
     def create_dataloader(path, imgsz, batch_size, stride, opt, hyp=None, augment=False, cache=False, pad=0.0, rect=False,                       rank=-1, world_size=1):                       rank=-1, world_size=1, workers=8):      # Make sure only the first process in DDP process the dataset first, and the following others can use the cache.      with torch_distributed_zero_first(rank):          dataset = LoadImagesAndLabels(path, imgsz, batch_size, 
