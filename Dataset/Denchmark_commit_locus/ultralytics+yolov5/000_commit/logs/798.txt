      hyp['anchors'] evolution update 
         with torch_distributed_zero_first(rank):              attempt_download(weights)  # download if not found locally          ckpt = torch.load(weights, map_location=device)  # load checkpoint         # if hyp['anchors']:         #     ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  # force autoanchor         if 'anchors' in hyp and hyp['anchors']:             ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  # force autoanchor          model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  # create          exclude = ['anchor'] if opt.cfg else []  # exclude keys          state_dict = ckpt['model'].float().state_dict()  # to FP32 
