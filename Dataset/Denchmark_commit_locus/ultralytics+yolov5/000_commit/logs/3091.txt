      Add `train.run()` method (#3700)          * Update train.py explicit arguments          * Update train.py          * Add run method 
         with torch_distributed_zero_first(RANK):              weights = attempt_download(weights)  # download if not found locally          ckpt = torch.load(weights, map_location=device)  # load checkpoint         model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create         exclude = ['anchor'] if (opt.cfg or hyp.get('anchors')) and not opt.resume else []  # exclude keys         model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create         exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys          state_dict = ckpt['model'].float().state_dict()  # to FP32          state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect          model.load_state_dict(state_dict, strict=False)  # load          logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  # report      else:         model = Model(opt.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create         model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create      with torch_distributed_zero_first(RANK):          check_dataset(data_dict)  # check      train_path = data_dict['train'] 
