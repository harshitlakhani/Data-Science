      Create one_cycle() function (#1836) 
       # Scheduler https://arxiv.org/pdf/1812.01187.pdf      # https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR     lf = lambda x: ((1 + math.cos(x * math.pi / epochs)) / 2) * (1 - hyp['lrf']) + hyp['lrf']  # cosine     lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']      scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)      # plot_lr_scheduler(optimizer, scheduler, epochs)        # Logging     if wandb and wandb.run is None:     if rank in [-1, 0] and wandb and wandb.run is None:          opt.hyp = hyp  # add hyperparameters          wandb_run = wandb.init(config=opt, resume="allow",                                 project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem, 
