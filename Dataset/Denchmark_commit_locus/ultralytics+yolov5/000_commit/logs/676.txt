      Single-source training (#680)          * Single-source training          * Extract hyperparameters into seperate files          * weight decay scientific notation yaml reader bug fix          * remove import glob          * intersect_dicts() implementation          * 'or' bug fix          * .to(device) bug fix 
     scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)      # plot_lr_scheduler(optimizer, scheduler, epochs)       # Load Model     with torch_distributed_zero_first(rank):         attempt_download(weights)     # Resume      start_epoch, best_fitness = 0, 0.0     if weights.endswith('.pt'):  # pytorch format         ckpt = torch.load(weights, map_location=device)  # load checkpoint          # load model         try:             exclude = ['anchor']  # exclude keys             ckpt['model'] = {k: v for k, v in ckpt['model'].float().state_dict().items()                              if k in model.state_dict() and not any(x in k for x in exclude)                              and model.state_dict()[k].shape == v.shape}             model.load_state_dict(ckpt['model'], strict=False)             print('Transferred %g/%g items from %s' % (len(ckpt['model']), len(model.state_dict()), weights))         except KeyError as e:             s = "%s is not compatible with %s. This may be due to model differences or %s may be out of date. " \                 "Please delete or update %s and try again, or use --weights '' to train from scratch." \                 % (weights, opt.cfg, weights, weights)             raise KeyError(s) from e          # load optimizer     if pretrained:         # Optimizer          if ckpt['optimizer'] is not None:              optimizer.load_state_dict(ckpt['optimizer'])              best_fitness = ckpt['best_fitness']           # load results         # Results          if ckpt.get('training_results') is not None:              with open(results_file, 'w') as file:                  file.write(ckpt['training_results'])  # write results.txt           # epochs         # Epochs          start_epoch = ckpt['epoch'] + 1          if epochs < start_epoch:              print('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %                    (weights, ckpt['epoch'], epochs))              epochs += ckpt['epoch']  # finetune additional epochs           del ckpt         del ckpt, state_dict        # DP mode      if cuda and rank == -1 and torch.cuda.device_count() > 1: 
