      Fix redundant outputs via Logging in DDP training (#500)          * Change print to logging          * Clean function set_logging          * Add line spacing          * Change leftover prints to log          * Fix scanning labels output          * Fix rank naming          * Change leftover print to logging          * Reorganized DDP variables          * Fix type error          * Make quotes consistent          * Fix spelling          * Clean function call          * Add line spacing          * Update datasets.py          * Update train.py          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
     parser.add_argument('--logdir', type=str, default='runs/', help='logging directory')      opt = parser.parse_args()       # Set DDP variables     opt.total_batch_size = opt.batch_size     opt.world_size = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1     opt.global_rank = int(os.environ['RANK']) if 'RANK' in os.environ else -1     set_logging(opt.global_rank)       # Resume      if opt.resume:          last = get_latest_run() if opt.resume == 'get_last' else opt.resume  # resume from most recent run          if last and not opt.weights:             print(f'Resuming training from {last}')             logger.info(f'Resuming training from {last}')          opt.weights = last if opt.resume and not opt.weights else opt.weights     if opt.local_rank == -1 or ("RANK" in os.environ and os.environ["RANK"] == "0"):     if opt.global_rank in [-1,0]:          check_git_status()        opt.hyp = opt.hyp or ('data/hyp.finetune.yaml' if opt.weights else 'data/hyp.scratch.yaml') 
