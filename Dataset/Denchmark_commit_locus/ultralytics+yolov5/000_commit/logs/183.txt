      Add TransformerLayer, TransformerBlock, C3TR modules (#2333)          * yolotr          * transformer block          * Remove bias in Transformer          * Remove C3T          * Remove a deprecated class          * put the 2nd LayerNorm into the 2nd residual block          * move example model to models/hub, rename to -transformer          * Add module comments and TODOs          * Remove LN in Transformer          * Add comments for Transformer          * Solve the problem of MA with DDP          * cleanup          * cleanup find_unused_parameters          * PEP8 reformat          Co-authored-by: DingYiwei <846414640@qq.com>     Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
         return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))     class C3TR(C3):     # C3 module with TransformerBlock()     def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):         super().__init__(c1, c2, n, shortcut, g, e)         c_ = int(c2 * e)         self.m = TransformerBlock(c_, c_, 4, n)    class SPP(nn.Module):      # Spatial pyramid pooling layer used in YOLOv3-SPP      def __init__(self, c1, c2, k=(5, 9, 13)): 
