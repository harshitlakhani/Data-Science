      torch.cuda.amp bug fix (#2750)          PR https://github.com/ultralytics/yolov5/pull/2725 introduced a very specific bug that only affects multi-GPU trainings. Apparently the cause was using the torch.cuda.amp decorator in the autoShape forward method. I've implemented amp more traditionally in this PR, and the bug is resolved. 
         t = [time_synchronized()]          p = next(self.model.parameters())  # for device and type          if isinstance(imgs, torch.Tensor):  # torch             return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference             with amp.autocast(enabled=p.device.type != 'cpu'):                 return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference            # Pre-process          n, imgs = (len(imgs), imgs) if isinstance(imgs, list) else (1, [imgs])  # number of images, list of images 
