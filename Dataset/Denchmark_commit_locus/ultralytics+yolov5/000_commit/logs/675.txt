      Single-source training (#680)          * Single-source training          * Extract hyperparameters into seperate files          * weight decay scientific notation yaml reader bug fix          * remove import glob          * intersect_dicts() implementation          * 'or' bug fix          * .to(device) bug fix 
     nc, names = (1, ['item']) if opt.single_cls else (int(data_dict['nc']), data_dict['names'])  # number classes, names      assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  # check       # Remove previous results     if rank in [-1, 0]:         for f in glob.glob('*_batch*.jpg') + glob.glob(results_file):             os.remove(f)      # Create model     model = Model(opt.cfg, nc=nc).to(device)      # Image sizes     gs = int(max(model.stride))  # grid size (max stride)     imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples     # Model     pretrained = weights.endswith('.pt')     if pretrained:         with torch_distributed_zero_first(rank):             attempt_download(weights)  # download if not found locally         ckpt = torch.load(weights, map_location=device)  # load checkpoint         model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  # create         exclude = ['anchor'] if opt.cfg else []  # exclude keys         state_dict = ckpt['model'].float().state_dict()  # to FP32         state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect         model.load_state_dict(state_dict, strict=False)  # load         print('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  # report     else:         model = Model(opt.cfg, ch=3, nc=nc).to(device)  # create        # Optimizer      nbs = 64  # nominal batch size     # default DDP implementation is slow for accumulation according to: https://pytorch.org/docs/stable/notes/ddp.html     # all-reduce operation is carried out during loss.backward().     # Thus, there would be redundant all-reduce communications in a accumulation procedure,     # which means, the result is still right but the training speed gets slower.     # TODO: If acceleration is needed, there is an implementation of allreduce_post_accumulation     # in https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/run_pretraining.py      accumulate = max(round(nbs / total_batch_size), 1)  # accumulate loss before optimizing      hyp['weight_decay'] *= total_batch_size * accumulate / nbs  # scale weight_decay        pg0, pg1, pg2 = [], [], []  # optimizer parameter groups      for k, v in model.named_parameters():         if v.requires_grad:             if '.bias' in k:                 pg2.append(v)  # biases             elif '.weight' in k and '.bn' not in k:                 pg1.append(v)  # apply weight decay             else:                 pg0.append(v)  # all else         v.requires_grad = True         if '.bias' in k:             pg2.append(v)  # biases         elif '.weight' in k and '.bn' not in k:             pg1.append(v)  # apply weight decay         else:             pg0.append(v)  # all else        if opt.adam:          optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum 
