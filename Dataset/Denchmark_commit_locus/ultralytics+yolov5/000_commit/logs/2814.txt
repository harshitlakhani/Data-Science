      Improved model+EMA checkpointing (#2292)          * Enhanced model+EMA checkpointing          * update          * bug fix          * bug fix 2          * always save optimizer          * ema half          * remove model.float()          * model half          * carry ema/model in fp32          * rm model.float()          * both to float always          * cleanup          * cleanup 
                 ckpt = {'epoch': epoch,                          'best_fitness': best_fitness,                          'training_results': results_file.read_text(),                         'model': ema.ema,                         'optimizer': None if final_epoch else optimizer.state_dict(),                         'model': (model.module if is_parallel(model) else model).half(),                         'ema': (ema.ema.half(), ema.updates),                         'optimizer': optimizer.state_dict(),                          'wandb_id': wandb_run.id if wandb else None}                    # Save last, best and delete 
