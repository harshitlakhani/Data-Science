      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
         cudnn.benchmark = True     def select_device(device='', apex=False, batch_size=None): def select_device(device='', batch_size=None):      # device = 'cpu' or '0' or '0,1,2,3'      cpu_request = device.lower() == 'cpu'      if device and not cpu_request:  # if device requested other than 'cpu' 
