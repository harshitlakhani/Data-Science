      Eliminate `total_batch_size` variable (#3697)          * Eliminate `total_batch_size` variable          * cleanup          * Update train.py 
       # Optimizer      nbs = 64  # nominal batch size     accumulate = max(round(nbs / total_batch_size), 1)  # accumulate loss before optimizing     hyp['weight_decay'] *= total_batch_size * accumulate / nbs  # scale weight_decay     accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing     hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay      logger.info(f"Scaled weight_decay = {hyp['weight_decay']}")        pg0, pg1, pg2 = [], [], []  # optimizer parameter groups 
