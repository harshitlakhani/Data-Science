      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
     def check_wandb_resume(opt):     process_wandb_config_ddp_mode(opt) if opt.global_rank not in [-1, 0] else None     process_wandb_config_ddp_mode(opt) if RANK not in [-1, 0] else None      if isinstance(opt.resume, str):          if opt.resume.startswith(WANDB_ARTIFACT_PREFIX):             if opt.global_rank not in [-1, 0]:  # For resuming DDP runs             if RANK not in [-1, 0]:  # For resuming DDP runs                  entity, project, run_id, model_artifact_name = get_run_info(opt.resume)                  api = wandb.Api()                  artifact = api.artifact(entity + '/' + project + '/' + model_artifact_name + ':latest') 
