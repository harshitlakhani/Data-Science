      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
     if opt.resume and not wandb_run:  # resume an interrupted run          ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path          assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'         apriori = opt.global_rank, opt.local_rank          with open(Path(ckpt).parent.parent / 'opt.yaml') as f:              opt = argparse.Namespace(**yaml.safe_load(f))  # replace         opt.cfg, opt.weights, opt.resume, opt.batch_size, opt.global_rank, opt.local_rank = \             '', ckpt, True, opt.total_batch_size, *apriori  # reinstate         opt.cfg, opt.weights, opt.resume, opt.batch_size = '', ckpt, True, opt.total_batch_size  # reinstate          logger.info('Resuming training from %s' % ckpt)      else:          # opt.hyp = opt.hyp or ('hyp.finetune.yaml' if opt.weights else 'hyp.scratch.yaml') 
