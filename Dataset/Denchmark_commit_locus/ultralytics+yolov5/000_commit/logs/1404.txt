      update train.py gsutil bucket fix (#463) 
             pred = model(imgs)                # Loss             loss, loss_items = compute_loss(pred, targets.to(device), model)             # loss is scaled with batch size in func compute_loss. But in DDP mode, gradient is averaged between devices.             if local_rank != -1:                 loss *= opt.world_size             loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size             if rank != -1:                 loss *= opt.world_size  # gradient averaged between devices in DDP mode              if not torch.isfinite(loss):                  print('WARNING: non-finite loss, ending training ', loss_items)                  return results 
