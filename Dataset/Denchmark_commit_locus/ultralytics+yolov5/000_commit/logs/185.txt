      Add TransformerLayer, TransformerBlock, C3TR modules (#2333)          * yolotr          * transformer block          * Remove bias in Transformer          * Remove C3T          * Remove a deprecated class          * put the 2nd LayerNorm into the 2nd residual block          * move example model to models/hub, rename to -transformer          * Add module comments and TODOs          * Remove LN in Transformer          * Add comments for Transformer          * Solve the problem of MA with DDP          * cleanup          * cleanup find_unused_parameters          * PEP8 reformat          Co-authored-by: DingYiwei <846414640@qq.com>     Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
       # DDP mode      if cuda and rank != -1:         model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)         model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank,                     # nn.MultiheadAttention incompatibility with DDP https://github.com/pytorch/pytorch/issues/26698                     find_unused_parameters=any(isinstance(layer, nn.MultiheadAttention) for layer in model.modules()))        # Model parameters      hyp['box'] *= 3. / nl  # scale to layers 
