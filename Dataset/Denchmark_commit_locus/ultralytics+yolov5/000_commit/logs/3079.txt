      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
     # DDP mode      opt.total_batch_size = opt.batch_size      device = select_device(opt.device, batch_size=opt.batch_size)     if opt.local_rank != -1:         assert torch.cuda.device_count() > opt.local_rank         torch.cuda.set_device(opt.local_rank)         device = torch.device('cuda', opt.local_rank)         dist.init_process_group(backend='nccl', init_method='env://')  # distributed backend         assert opt.batch_size % opt.world_size == 0, '--batch-size must be multiple of CUDA device count'     if LOCAL_RANK != -1:         from datetime import timedelta         assert torch.cuda.device_count() > LOCAL_RANK, 'too few GPUS for DDP command'         torch.cuda.set_device(LOCAL_RANK)         device = torch.device('cuda', LOCAL_RANK)         dist.init_process_group(backend="gloo", timeout=timedelta(seconds=60))         assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'          assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'         opt.batch_size = opt.total_batch_size // opt.world_size         opt.batch_size = opt.total_batch_size // WORLD_SIZE        # Train     logger.info(opt)      if not opt.evolve:          train(opt.hyp, opt, device)         if WORLD_SIZE > 1 and RANK == 0:             _ = [print('Destroying process group... ', end=''), dist.destroy_process_group(), print('Done.')]        # Evolve hyperparameters (optional)      else: 
