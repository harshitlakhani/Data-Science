      PyTorch 1.6.0 update with native AMP (#573)          * PyTorch have Automatic Mixed Precision (AMP) Training.          * Fixed the problem of inconsistent code length indentation          * Fixed the problem of inconsistent code length indentation          * Mixed precision training is turned on by default 
         with open(opt.hyp) as f:              hyp.update(yaml.load(f, Loader=yaml.FullLoader))  # update hyps      opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)     device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)     device = torch_utils.select_device(opt.device, batch_size=opt.batch_size)      opt.total_batch_size = opt.batch_size      opt.world_size = 1     if device.type == 'cpu':         mixed_precision = False     elif opt.local_rank != -1:         # DDP mode      # DDP mode     if opt.local_rank != -1:          assert torch.cuda.device_count() > opt.local_rank          torch.cuda.set_device(opt.local_rank)          device = torch.device("cuda", opt.local_rank)          dist.init_process_group(backend='nccl', init_method='env://')  # distributed backend           opt.world_size = dist.get_world_size()          assert opt.batch_size % opt.world_size == 0, "Batch size is not a multiple of the number of devices given!"          opt.batch_size = opt.total_batch_size // opt.world_size       print(opt)        # Train 
