      Change optimizer parameters group method (#1239)          * Change optimizer parameters group method          * Add torch nn          * Change isinstance method(torch.Tensor to nn.Parameter)          * parameter freeze fix, PEP8 reformat          * freeze bug fix          Co-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com> 
         model = Model(opt.cfg, ch=3, nc=nc).to(device)  # create        # Freeze     freeze = ['', ]  # parameter names to freeze (full or partial)     if any(freeze):         for k, v in model.named_parameters():             if any(x in k for x in freeze):                 print('freezing %s' % k)                 v.requires_grad = False     freeze = []  # parameter names to freeze (full or partial)     for k, v in model.named_parameters():         v.requires_grad = True  # train all layers         if any(x in k for x in freeze):             print('freezing %s' % k)             v.requires_grad = False        # Optimizer      nbs = 64  # nominal batch size 
