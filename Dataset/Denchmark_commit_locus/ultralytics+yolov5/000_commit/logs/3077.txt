      Update DDP for `torch.distributed.run` with `gloo` backend (#3680)          * Update DDP for `torch.distributed.run`          * Add LOCAL_RANK          * remove opt.local_rank          * backend="gloo|nccl"          * print          * print          * debug          * debug          * os.getenv          * gloo          * gloo          * gloo          * cleanup          * fix getenv          * cleanup          * cleanup destroy          * try nccl          * return opt          * add --local_rank          * add timeout          * add init_method          * gloo          * move destroy          * move destroy          * move print(opt) under if RANK          * destroy only RANK 0          * move destroy inside train()          * restore destroy outside train()          * update print(opt)          * cleanup          * nccl          * gloo with 60 second timeout          * update namespace printing 
     parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval for W&B')      parser.add_argument('--save_period', type=int, default=-1, help='Log model after every "save_period" epoch')      parser.add_argument('--artifact_alias', type=str, default="latest", help='version of dataset artifact to be used')     parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')      opt = parser.parse_args()      # Set DDP variables     opt.world_size = int(getattr(os.environ, 'WORLD_SIZE', 1))     opt.global_rank = int(getattr(os.environ, 'RANK', -1))      return opt      def main(opt):     print(opt)     set_logging(opt.global_rank)     if opt.global_rank in [-1, 0]:     set_logging(RANK)     if RANK in [-1, 0]:         print(colorstr('train: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))          check_git_status()          check_requirements(exclude=['thop'])   
