      Fix yaml saving (don't sort keys), reorder --opt keys, bug fix hyp dict accessor 
             else:                  pg0.append(v)  # all else       if hyp.optimizer =='adam':     if hyp['optimizer'] =='adam':          optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999)) #use default beta2, adjust beta1 for Adam momentum per momentum adjustments in https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR      else:          optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True) 
