torch cuda amp bug fix https github com ultralytics yolov pull introduced specific bug affects multi gpu multigpu trainings apparently cause using torch cuda amp decorator auto shape autoshape forward method implemented amp traditionally bug resolved
torch numpy fromnumpy device type typeas uint append time synchronized timesynchronized inference self model augment profile forward append time synchronized timesynchronized amp autocast enabled device type cpu inference self model augment profile forward append time synchronized timesynchronized post process postprocess non max suppression nonmaxsuppression conf thres confthres self conf iou thres iouthres self iou classes self classes nms range scale coords scalecoords shape shapei post process postprocess non max suppression nonmaxsuppression conf thres confthres self conf iou thres iouthres self iou classes self classes nms range scale coords scalecoords shape shapei append time synchronized timesynchronized return detections imgs files self names shape append time synchronized timesynchronized return detections imgs files self names shape class detections
