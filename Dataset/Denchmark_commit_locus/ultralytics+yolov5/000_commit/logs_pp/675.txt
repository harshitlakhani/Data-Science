single source singlesource training single source singlesource training extract hyperparameters seperate files weight decay scientific notation yaml reader bug fix remove import glob intersect dicts intersectdicts implementation bug fix device bug fix
names item opt single cls singlecls else int data dict datadict data dict datadict names number classes names assert len names names found dataset len names opt data check remove previous results rank glob glob batch jpg glob glob results file resultsfile remove create model model model opt cfg device image sizes int max model stride grid size max stride imgsz imgsz test imgsztest check img size checkimgsize opt img size imgsize verify imgsz multiples gsmultiples model pretrained weights endswith pretrained torch distributed zero first torchdistributedzerofirst rank attempt download attemptdownload weights download found locally ckpt torch load weights map location maplocation device load checkpoint model model opt cfg ckpt model yaml device create exclude anchor opt cfg else exclude keys state dict statedict ckpt model float state dict statedict state dict statedict intersect dicts intersectdicts state dict statedict model state dict statedict exclude exclude intersect model load state dict loadstatedict state dict statedict strict false load print transferred items len state dict statedict len model state dict statedict weights report else model model opt cfg device create optimizer nbs nominal batch size default ddp implementation slow accumulation according https pytorch org docs stable notes ddp html reduce allreduce operation carried loss backward thus would redundant reduce allreduce communications accumulation procedure means result still right training speed gets slower todo acceleration needed implementation allreduce post accumulation allreducepostaccumulation https github com nvidia deep learning examples deeplearningexamples blob master torch pytorch language modeling languagemodeling bert run pretraining runpretraining accumulate max round nbs total batch size totalbatchsize accumulate loss optimizing hyp weight decay weightdecay total batch size totalbatchsize accumulate nbs scale weight decay weightdecay optimizer parameter groups model named parameters namedparameters requires grad requiresgrad bias append biases elif weight append apply weight decay else append else requires grad requiresgrad true bias append biases elif weight append apply weight decay else append else opt adam optimizer optim adam hyp betas hyp momentum adjust beta momentum
