torch cuda amp bug fix https github com ultralytics yolov pull introduced specific bug affects multi gpu multigpu trainings apparently cause using torch cuda amp decorator auto shape autoshape forward method implemented amp traditionally bug resolved
time synchronized timesynchronized next self model parameters device type isinstance imgs torch tensor torch return self model imgs device type typeas augment profile inference amp autocast enabled device type cpu return self model imgs device type typeas augment profile inference pre process preprocess imgs len imgs imgs isinstance imgs list else imgs number images list images
