merge branch master advanced logging advancedlogging
else append else optimizer optim adam hyp opt adam else optim sgd hyp momentum hyp momentum nesterov true hyp optimizer adam optimizer optim adam hyp betas hyp momentum use default beta adjust beta adam momentum per momentum adjustments https pytorch org docs stable modules torch optim scheduler lrscheduler html one cycle htmlonecyclelr else optimizer optim sgd hyp momentum hyp momentum nesterov true optimizer add param group addparamgroup params weight decay weightdecay hyp weight decay weightdecay add weight decay weightdecay optimizer add param group addparamgroup params add biases scheduler https arxiv org pdf pdf lambda math cos math epochs cosine scheduler scheduler lrscheduler lambda lambdalr optimizer lambda lrlambda print optimizer groups bias conv weight len len len del
