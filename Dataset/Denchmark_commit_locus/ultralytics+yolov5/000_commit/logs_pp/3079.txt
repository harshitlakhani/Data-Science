update ddp torch distributed run gloo backend update ddp torch distributed run add localrank remove opt local rank localrank backend gloo nccl print print debug debug getenv gloo gloo gloo cleanup fix getenv cleanup cleanup destroy try nccl return opt add local rank localrank add timeout add init method initmethod gloo move destroy move destroy move print opt rank destroy rank move destroy inside train restore destroy outside train update print opt cleanup nccl gloo second timeout update namespace printing
ddp mode opt total batch size totalbatchsize opt batch size batchsize device select device selectdevice opt device batch size batchsize opt batch size batchsize opt local rank localrank assert torch cuda device count devicecount opt local rank localrank torch cuda set device setdevice opt local rank localrank device torch device cuda opt local rank localrank dist init process group initprocessgroup backend nccl init method initmethod env distributed backend assert opt batch size batchsize opt world size worldsize batch size batchsize must multiple cuda device count localrank datetime import timedelta assert torch cuda device count devicecount localrank gpus ddp command torch cuda set device setdevice localrank device torch device cuda localrank dist init process group initprocessgroup backend gloo timeout timedelta seconds assert opt batch size batchsize worldsize batch size batchsize must multiple cuda device count assert opt image weights imageweights image weights imageweights argument compatible ddp training opt batch size batchsize opt total batch size totalbatchsize opt world size worldsize opt batch size batchsize opt total batch size totalbatchsize worldsize train logger info opt opt evolve train opt hyp opt device worldsize rank print destroying process group end dist destroy process group destroyprocessgroup print done evolve hyperparameters optional else
